{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "\n",
    "import joblib\n",
    "import natsort\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     14,
     40,
     71,
     103
    ]
   },
   "outputs": [],
   "source": [
    "def get_clusters_falcon(filename, ids=None):\n",
    "    cluster_labels = pd.read_csv(filename, comment='#')\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = pd.merge(cluster_labels,\n",
    "                                  ids[['identifier', 'sequence']],\n",
    "                                  'left', 'identifier')\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels\n",
    "    \n",
    "\n",
    "def get_clusters_maracluster(filename, ids=None):\n",
    "    cluster_labels = (pd.read_csv(filename, sep='\\t',\n",
    "                                  names=['filename', 'scan', 'cluster'])\n",
    "                      .dropna(how='all'))\n",
    "    cluster_labels['identifier'] = (\n",
    "        'mzspec:PXD000561:'\n",
    "        + cluster_labels['filename'].apply(\n",
    "            lambda fn: os.path.splitext(os.path.basename(fn))[0])\n",
    "        + ':scan:' + cluster_labels['scan'].astype(str))\n",
    "    cluster_labels = cluster_labels[['identifier', 'cluster']]\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(cluster_labels,\n",
    "                                   ids[['identifier', 'precursor_charge',\n",
    "                                        'precursor_mz', 'sequence']],\n",
    "                                   'left', 'identifier')\n",
    "                           .dropna(subset=['precursor_charge']))\n",
    "        cluster_labels['precursor_charge'] = \\\n",
    "            cluster_labels['precursor_charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels\n",
    "    \n",
    "    \n",
    "def get_clusters_mscluster(dir_name, ids=None):\n",
    "    clusters, cluster_i = [], -1\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename.endswith('.clust'):\n",
    "            with open(os.path.join(dir_name, filename)) as f_in:\n",
    "                for line in f_in:\n",
    "                    if line.startswith('mscluster'):\n",
    "                        cluster_i += 1\n",
    "                    elif not line.isspace():\n",
    "                        splits = line.split('\\t')\n",
    "                        file_i = int(splits[1])\n",
    "                        spectrum_i = int(splits[2])\n",
    "                        clusters.append((file_i, spectrum_i, cluster_i))\n",
    "    cluster_labels = pd.DataFrame(clusters, columns=['file_i', 'spectrum_i',\n",
    "                                                     'cluster'])\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(cluster_labels, ids,\n",
    "                                   'left', ['file_i', 'spectrum_i'])\n",
    "                          .dropna(subset=['precursor_charge'])\n",
    "                          [['identifier', 'cluster', 'precursor_charge',\n",
    "                            'precursor_mz', 'sequence']])\n",
    "        cluster_labels['precursor_charge'] = \\\n",
    "            cluster_labels['precursor_charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_mscrush(dir_name, ids=None):\n",
    "    cluster_labels = []\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename.endswith('.txt'):\n",
    "            clusters_file = pd.read_csv(os.path.join(dir_name, filename),\n",
    "                                        sep='\\t')\n",
    "            clusters_file['Titles'] = clusters_file['Titles'].str.split('|')\n",
    "            clusters_file = clusters_file.explode('Titles')\n",
    "            clusters_file['identifier'] = ('mzspec:PXD000561:'\n",
    "                                           + clusters_file['Titles'])\n",
    "            clusters_file = clusters_file.rename(columns={'ID': 'cluster'})\n",
    "            clusters_file = clusters_file[['identifier', 'cluster']]\n",
    "            if len(cluster_labels) > 0:\n",
    "                clusters_file['cluster'] += cluster_labels[-1].iat[-1, 1] + 1\n",
    "            cluster_labels.append(clusters_file)\n",
    "    cluster_labels = pd.concat(cluster_labels, ignore_index=True)\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(cluster_labels,\n",
    "                                   ids[['identifier', 'precursor_charge',\n",
    "                                        'precursor_mz', 'sequence']],\n",
    "                                   'left', 'identifier')\n",
    "                           .dropna(subset=['precursor_charge']))\n",
    "        cluster_labels['precursor_charge'] = \\\n",
    "            cluster_labels['precursor_charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_spectracluster(filename, ids=None):\n",
    "    identifiers, clusters, cluster_i = [], [], -1\n",
    "    with open(filename) as f_in:\n",
    "        for line in f_in:\n",
    "            if line.startswith('=Cluster='):\n",
    "                cluster_i += 1\n",
    "            elif line.startswith('SPEC'):\n",
    "                fn_start_i = line.find('interim/') + len('interim/')\n",
    "                fn_stop_i = line.find('.mgf', fn_start_i)\n",
    "                scan_start_i = line.find('scan=') + len('scan=')\n",
    "                scan_stop_i = line.find('\\t', scan_start_i)\n",
    "                identifiers.append(f'mzspec:PXD000561:'\n",
    "                                   f'{line[fn_start_i:fn_stop_i]}:scan:'\n",
    "                                   f'{line[scan_start_i:scan_stop_i]}')\n",
    "                clusters.append(cluster_i)\n",
    "    cluster_labels = pd.DataFrame({'identifier': identifiers,\n",
    "                                   'cluster': clusters})\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(cluster_labels,\n",
    "                                   ids[['identifier', 'precursor_charge',\n",
    "                                        'precursor_mz', 'sequence']],\n",
    "                                   'left', 'identifier')\n",
    "                          .dropna(subset=['precursor_charge']))\n",
    "        cluster_labels['precursor_charge'] = \\\n",
    "            cluster_labels['precursor_charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_parquet('kim2014_ids.parquet')\n",
    "ids['sequence'] = ids['sequence'].str.replace('L', 'I')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze cluster sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.read_csv('cluster_comparison_hyperparam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the \"best\" performing runs (low number of incorrectly clustered spectra).\n",
    "target_incorrect, tool_index = 0.01, {}\n",
    "for tool, tool_performance in \\\n",
    "        ((performance.set_index('tool')['prop_clustered_incorrect'] - target_incorrect)\n",
    "         .abs().reset_index().groupby('tool')):\n",
    "    idxmin = (tool_performance['prop_clustered_incorrect'] ==\n",
    "              tool_performance['prop_clustered_incorrect'].min())\n",
    "    tool_index[tool] = (np.where(idxmin)[0][0],\n",
    "                        tool_performance[idxmin].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance.loc[[idx[1] for idx in tool_index.values()]].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read clustering results from the different tools.\n",
    "tool_clusters = {\n",
    "    'falcon': get_clusters_falcon(\n",
    "        f'../data/processed/falcon/PXD000561_falcon_'\n",
    "        f'{tool_index[\"falcon\"][0]}.csv', ids),\n",
    "    'MaRaCluster': get_clusters_maracluster(\n",
    "        f'../data/processed/maracluster/PXD000561_maracluster_'\n",
    "        f'{tool_index[\"MaRaCluster\"][0]}.tsv', ids),\n",
    "    'MS-Cluster': get_clusters_mscluster(\n",
    "        f'../data/processed/mscluster/PXD000561_mscluster_'\n",
    "        f'{tool_index[\"MS-Cluster\"][0]}', ids),\n",
    "    'msCRUSH': get_clusters_mscrush(\n",
    "        f'../data/processed/mscrush/PXD000561_mscrush_'\n",
    "        f'{tool_index[\"msCRUSH\"][0]}', ids),\n",
    "    'spectra-cluster': get_clusters_spectracluster(\n",
    "        f'../data/processed/spectra-cluster/PXD000561_spectra-cluster_'\n",
    "        f'{tool_index[\"spectra-cluster\"][0]}.txt', ids)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove singleton and noise clusters.\n",
    "min_cluster_size, max_cluster_size = 2, None\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    # Use consecutive cluster labels, skipping the noise points.    \n",
    "    cluster_map = clusters['cluster'].value_counts(dropna=False)\n",
    "    if -1 in cluster_map.index:\n",
    "        cluster_map = cluster_map.drop(index=-1)\n",
    "    cluster_map = (cluster_map.to_frame().reset_index().reset_index()\n",
    "                   .rename(columns={'index': 'old', 'level_0': 'new'})\n",
    "                   .set_index('old')['new'])\n",
    "    cluster_map = cluster_map.to_dict(collections.defaultdict(lambda: -1))\n",
    "    clusters['cluster'] = clusters['cluster'].map(cluster_map)\n",
    "\n",
    "    # Reassign noise points to singleton clusters.\n",
    "    noise_mask = clusters['cluster'] == -1\n",
    "    num_clusters = clusters['cluster'].max() + 1\n",
    "    clusters.loc[noise_mask, 'cluster'] = np.arange(\n",
    "        num_clusters, num_clusters + noise_mask.sum())\n",
    "    \n",
    "    tool_clusters[tool] = clusters\n",
    "    \n",
    "# Add cluster sizes.\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    cluster_counts = (clusters['cluster']\n",
    "                      .value_counts(dropna=False)\n",
    "                      .to_frame()\n",
    "                      .reset_index()\n",
    "                      .rename(columns={'index': 'cluster', 'cluster': 'size'}))\n",
    "    tool_clusters[tool] = pd.merge(clusters, cluster_counts, on='cluster')\n",
    "    \n",
    "max_size = max([clusters['size'].max() for clusters in tool_clusters.values()])\n",
    "cluster_sizes = {tool: np.insert(clusters['size'].values, -1, max_size)\n",
    "                 for tool, clusters in tool_clusters.items()}\n",
    "_ = joblib.dump(cluster_sizes, 'cluster_comparison_size.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of clusters per tool:')\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    print(f'- {tool}: {clusters[clusters[\"size\"] > 1][\"cluster\"].nunique():,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_peptide = tool_clusters['falcon']['sequence'].value_counts()\n",
    "max_peptide, num_max_peptide = max_peptide.index.values[0], max_peptide.values[0]\n",
    "print(f'Most frequent peptide: {max_peptide} is observed '\n",
    "      f'{num_max_peptide:,d} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size_intervals = [(2, 5), (5, 20), (20, 100), (100, 500),\n",
    "                          (500, 5000), (5000, None)]\n",
    "max_peptide_clusters = []\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    for cluster in (clusters[clusters['sequence'] == max_peptide]\n",
    "                    ['cluster'].unique()):\n",
    "        sequence_counts = (clusters[clusters['cluster'] == cluster]\n",
    "                           ['sequence'].value_counts(dropna=False))\n",
    "        if sequence_counts.sum() < cluster_size_intervals[0][0]:\n",
    "            continue\n",
    "        num_correct = sequence_counts[max_peptide]\n",
    "        num_unidentified = (sequence_counts[np.NaN]\n",
    "                            if np.NaN in sequence_counts else 0)\n",
    "        num_incorrect = sequence_counts.sum() - num_correct - num_unidentified\n",
    "        # Only consider clusters where this is the majority peptide.\n",
    "        if num_correct > num_incorrect:\n",
    "            cluster_size = num_correct + num_incorrect + num_unidentified\n",
    "            for min_interval_size, max_interval_size in cluster_size_intervals:\n",
    "                if (max_interval_size is None\n",
    "                        and min_interval_size <= cluster_size):\n",
    "                    interval = f'{min_interval_size}+'\n",
    "                    break\n",
    "                elif min_interval_size <= cluster_size < max_interval_size:\n",
    "                    interval = f'{min_interval_size}â€“{max_interval_size}'\n",
    "                    break\n",
    "            max_peptide_clusters.append((tool, interval, num_correct,\n",
    "                                         num_unidentified, num_incorrect))\n",
    "max_peptide_clusters = (\n",
    "    pd.DataFrame(max_peptide_clusters, columns=[\n",
    "        'tool', 'interval', 'num_correct', 'num_unidentified', 'num_incorrect'])\n",
    "    .sort_values(['tool', 'interval'], key=natsort.natsort_keygen()))\n",
    "max_peptide_clusters['num_total'] = (max_peptide_clusters['num_correct'] +\n",
    "                                     max_peptide_clusters['num_unidentified'] +\n",
    "                                     max_peptide_clusters['num_incorrect'])\n",
    "\n",
    "max_peptide_clusters_grouped = pd.concat(\n",
    "    [max_peptide_clusters,\n",
    "     pd.DataFrame([(tool, interval, 0, 0, 0)\n",
    "                   for tool in max_peptide_clusters['tool'].unique()\n",
    "                   for interval in max_peptide_clusters['interval'].unique()],\n",
    "                  columns=['tool', 'interval', 'num_correct',\n",
    "                           'num_unidentified', 'num_incorrect'])],\n",
    "    ignore_index=True)    \n",
    "max_peptide_clusters_grouped = (max_peptide_clusters_grouped\n",
    "                                .sort_values(['tool', 'interval'],\n",
    "                                             key=natsort.natsort_keygen())\n",
    "                                .groupby(['tool', 'interval'], sort=False)\n",
    "                                [['num_correct', 'num_unidentified',\n",
    "                                  'num_incorrect']].apply(sum)\n",
    "                                .reset_index())\n",
    "max_peptide_clusters_grouped.to_csv('cluster_comparison_size.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of the top 5 largest clusters for peptide {max_peptide} per tool:')\n",
    "(max_peptide_clusters.groupby('tool')\n",
    " ['num_total'].apply(lambda x: x.sort_values(ascending=False).head())\n",
    " .to_frame().droplevel(1).reset_index()\n",
    " .rename(columns={'num_total': 'largest clusters'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique clusters for peptide {max_peptide} per tool:')\n",
    "for tool, num_clusters in max_peptide_clusters['tool'].value_counts().iteritems():\n",
    "    print(f'- {tool}: {num_clusters}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
