{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Make sure all code is in the PATH.\n",
    "sys.path.append(os.path.normpath(os.path.join('../src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import natsort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.metrics import homogeneity_score, completeness_score\n",
    "\n",
    "import config, falcon\n",
    "from ms_io import ms_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot styling.\n",
    "plt.style.use(['seaborn-white', 'seaborn-paper'])\n",
    "plt.rc('font', family='serif')\n",
    "sns.set_palette(['#6da7de', '#9e0059', '#dee000', '#d82222', '#5ea15d',\n",
    "                 '#943fa6', '#63c5b5', '#ff38ba', '#eb861e', '#ee266d'])\n",
    "sns.set_context('paper', font_scale=1.3)    # Single-column figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget --timestamping \\\n",
    "    --retry-connrefused \\\n",
    "    --directory-prefix=../data/external \\\n",
    "    --passive-ftp \\\n",
    "    ftp://ftp.pride.ebi.ac.uk/pride/data/archive/2014/04/PXD000561/*.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p ../data/interim\n",
    "\n",
    "for raw_file in ../data/external/*.raw; do\n",
    "    if [ ! -f ../data/interim/$(basename $raw_file .raw).mgf ]; then\n",
    "        ThermoRawFileParser -i $raw_file -o ../data/interim -f 0\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify MGF spectrum titles for compatibility with msCRUSH.\n",
    "mgf_dir = '../data/interim/'\n",
    "for filename in os.listdir(mgf_dir):\n",
    "    if filename.endswith('.mgf'):\n",
    "        filename = os.path.join(mgf_dir, filename)\n",
    "        spectra = list(ms_io.get_spectra(filename))\n",
    "        ms_io.write_spectra(filename, spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_sizes = [(2, None)]    #, (5, None), (10, None), (50, None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _count_majority_label_mismatch(labels):\n",
    "    labels_assigned = labels.dropna()\n",
    "    if len(labels_assigned) <= 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(labels_assigned) - labels_assigned.value_counts().iat[0]\n",
    "\n",
    "\n",
    "def evaluate_clusters(clusters, min_cluster_size=None, max_cluster_size=None):\n",
    "    clusters = clusters.copy()\n",
    "    # Only consider clusters with specific minimum (inclusive) and/or\n",
    "    # maximum (exclusive) size.\n",
    "    cluster_counts = clusters['cluster'].value_counts(dropna=False)\n",
    "    if min_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts < min_cluster_size].index), 'cluster'] = -1\n",
    "    if max_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts >= max_cluster_size].index), 'cluster'] = -1\n",
    "\n",
    "    # Use consecutive cluster labels, skipping the noise points.    \n",
    "    cluster_map = (clusters['cluster'].value_counts(dropna=False)\n",
    "                   .drop(index=-1).to_frame().reset_index().reset_index()\n",
    "                   .rename(columns={'index': 'old', 'level_0': 'new'})\n",
    "                   .set_index('old')['new'])\n",
    "    cluster_map = cluster_map.to_dict(collections.defaultdict(lambda: -1))\n",
    "    clusters['cluster'] = clusters['cluster'].map(cluster_map)\n",
    "    num_clusters = clusters['cluster'].max() + 1\n",
    "\n",
    "    # Reassign noise points to singleton clusters.\n",
    "    noise_mask = clusters['cluster'] == -1\n",
    "    num_noise = noise_mask.sum()\n",
    "    clusters.loc[noise_mask, 'cluster'] = np.arange(\n",
    "        num_clusters, num_clusters + num_noise)\n",
    "\n",
    "    # Compute cluster evaluation measures.\n",
    "    prop_clustered = (len(clusters) - num_noise) / len(clusters)\n",
    "\n",
    "    clusters_ident = clusters.dropna(subset=['sequence'])\n",
    "    clusters_ident_non_noise = (clusters[~noise_mask]\n",
    "                                .dropna(subset=['sequence']))\n",
    "\n",
    "    # The number of incorrectly clustered spectra is the number of PSMs that\n",
    "    # differ from the majority PSM. Unidentified spectra are not considered.\n",
    "    prop_clustered_incorrect = sum(joblib.Parallel(n_jobs=-1)(\n",
    "        joblib.delayed(_count_majority_label_mismatch)(clust['sequence'])\n",
    "        for _, clust in clusters[~noise_mask].groupby('cluster')))\n",
    "    prop_clustered_incorrect /= len(clusters_ident_non_noise)\n",
    "\n",
    "    # Homogeneity measures whether clusters contain only identical PSMs.\n",
    "    # This is only evaluated on non-noise points, because the noise cluster\n",
    "    # is highly non-homogeneous by definition.\n",
    "    homogeneity = homogeneity_score(clusters_ident_non_noise['sequence'],\n",
    "                                    clusters_ident_non_noise['cluster'])\n",
    "    # Completeness measures whether identical PSMs are assigned to the same\n",
    "    # cluster.\n",
    "    # This is evaluated on all PSMs, including those clustered as noise.\n",
    "    completeness = completeness_score(clusters_ident['sequence'],\n",
    "                                      clusters_ident['cluster'])\n",
    "\n",
    "    return (len(clusters) - num_noise, num_noise,\n",
    "            prop_clustered, prop_clustered_incorrect,\n",
    "            homogeneity, completeness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_mscluster(dir_name, ids=None):\n",
    "    clusters, cluster_i = [], -1\n",
    "    for filename in os.listdir(dir_name):\n",
    "        if filename.endswith('.clust'):\n",
    "            with open(os.path.join(dir_name, filename)) as f_in:\n",
    "                for line in f_in:\n",
    "                    if line.startswith('mscluster'):\n",
    "                        cluster_i += 1\n",
    "                    elif not line.isspace():\n",
    "                        splits = line.split('\\t')\n",
    "                        file_i = int(splits[1])\n",
    "                        spectrum_i = int(splits[2])\n",
    "                        clusters.append((file_i, spectrum_i, cluster_i))\n",
    "    cluster_labels = pd.DataFrame(clusters, columns=['file_i', 'spectrum_i',\n",
    "                                                     'cluster'])\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(cluster_labels, ids,\n",
    "                                   'left', ['file_i', 'spectrum_i'])\n",
    "                          .dropna(subset=['precursor_charge'])\n",
    "                          [['identifier', 'cluster', 'precursor_charge',\n",
    "                            'precursor_mz', 'sequence']])\n",
    "        cluster_labels['precursor_charge'] = \\\n",
    "            cluster_labels['precursor_charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_spectracluster(filename, ids=None):\n",
    "    identifiers, clusters, cluster_i = [], [], -1\n",
    "    with open(filename) as f_in:\n",
    "        for line in f_in:\n",
    "            if line.startswith('=Cluster='):\n",
    "                cluster_i += 1\n",
    "            elif line.startswith('SPEC'):\n",
    "                fn_start_i = line.find('interim/') + len('interim/')\n",
    "                fn_stop_i = line.find('.mgf', fn_start_i)\n",
    "                scan_start_i = line.find('scan=') + len('scan=')\n",
    "                scan_stop_i = line.find('\\t', scan_start_i)\n",
    "                identifiers.append(f'mzspec:PXD000561:'\n",
    "                                   f'{line[fn_start_i:fn_stop_i]}:scan:'\n",
    "                                   f'{line[scan_start_i:scan_stop_i]}')\n",
    "                clusters.append(cluster_i)\n",
    "    cluster_labels = pd.DataFrame({'identifier': identifiers,\n",
    "                                   'cluster': clusters})\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = (pd.merge(cluster_labels,\n",
    "                                   ids[['identifier', 'precursor_charge',\n",
    "                                        'precursor_mz', 'sequence']],\n",
    "                                   'left', 'identifier')\n",
    "                          .dropna(subset=['precursor_charge']))\n",
    "        cluster_labels['precursor_charge'] = \\\n",
    "            cluster_labels['precursor_charge'].astype(int)\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels\n",
    "\n",
    "\n",
    "def get_clusters_falcon(filename, ids=None):\n",
    "    cluster_labels = pd.read_csv(filename)\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = pd.merge(cluster_labels,\n",
    "                                  ids[['identifier', 'sequence']],\n",
    "                                  'left', 'identifier')\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels\n",
    "    \n",
    "\n",
    "def get_clusters_maracluster(filename, ids=None):\n",
    "    cluster_labels = (pd.read_csv(filename, sep='\\t',\n",
    "                                  names=['filename', 'scan', 'cluster'])\n",
    "                      .dropna(how='all'))\n",
    "    cluster_labels['identifier'] = (\n",
    "        'mzspec:PXD000561:'\n",
    "        + cluster_labels['filename'].apply(\n",
    "            lambda fn: os.path.splitext(os.path.basename(fn))[0])\n",
    "        + ':scan:' + cluster_labels['scan'].astype(str))\n",
    "    cluster_labels = cluster_labels[['identifier', 'cluster']]\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = pd.merge(cluster_labels,\n",
    "                                  ids[['identifier', 'precursor_charge',\n",
    "                                       'precursor_mz', 'sequence']],\n",
    "                                  'left', 'identifier')\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels\n",
    "    \n",
    "    \n",
    "def get_clusters_mscrush(filename, ids=None):\n",
    "    cluster_labels = []\n",
    "    for charge in config.charges:\n",
    "        clusters_charge = pd.read_csv(filename.format(charge), sep='\\t')\n",
    "        clusters_charge['Titles'] = clusters_charge['Titles'].str.split('|')\n",
    "        clusters_charge = clusters_charge.explode('Titles')\n",
    "        clusters_charge['identifier'] = ('mzspec:PXD000561:'\n",
    "                                         + clusters_charge['Titles'])\n",
    "        clusters_charge = clusters_charge.rename(columns={'ID': 'cluster'})\n",
    "        clusters_charge = clusters_charge[['identifier', 'cluster']]\n",
    "        if len(cluster_labels) > 0:\n",
    "            clusters_charge['cluster'] += cluster_labels[-1].iat[-1, 1] + 1\n",
    "        cluster_labels.append(clusters_charge)\n",
    "    cluster_labels = pd.concat(cluster_labels, ignore_index=True)\n",
    "    if ids is None:\n",
    "        return cluster_labels\n",
    "    else:\n",
    "        cluster_labels = pd.merge(cluster_labels,\n",
    "                                  ids[['identifier', 'precursor_charge',\n",
    "                                       'precursor_mz', 'sequence']],\n",
    "                                  'left', 'identifier')\n",
    "        cluster_labels['sequence'] = (\n",
    "            cluster_labels['sequence'] + '/' +\n",
    "            cluster_labels['precursor_charge'].astype(str))\n",
    "        return cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_parquet('kim2014_ids.parquet')\n",
    "ids['sequence'] = ids['sequence'].str.replace('L', 'I')\n",
    "ids = ids[ids['precursor_charge'].isin(config.charges)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MS-Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_mscluster = os.path.join(work_dir, 'mscluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p $../data/processed/mscluster\n",
    "realpath ../data/interim/*.mgf > ../data/processed/mscluster/mscluster_spec_list.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "time ../bin/MsCluster/MsCluster \\\n",
    "    --model LTQ_TRYP \\\n",
    "    --list ../data/processed/mscluster/mscluster_spec_list.txt \\\n",
    "    --output-name mscluster \\\n",
    "    --tmp-dir ../data/processed/mscluster/dat \\\n",
    "    --out-dir ../data/processed/mscluster \\\n",
    "    --dat-only \\\n",
    "    --model-dir ../bin/MsCluster/Models \\\n",
    "    --keep-dat \\\n",
    "    --assign-charges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mscluster = {0: 0.0001, 1: 0.001, 2: 0.005, 3: 0.01, 4: 0.05, 5: 0.1,\n",
    "                6: 0.2}\n",
    "rounds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mixture_prob in hp_mscluster.items():\n",
    "    logging.info('MS-Cluster run %d (mixture-prob=%.3f ; num-rounds=%d)',\n",
    "                 i + 1, mixture_prob, rounds)\n",
    "    # Execute clustering.\n",
    "    cmd = f\"\"\"../bin/MsCluster/MsCluster \\\n",
    "        --model LTQ_TRYP \\\n",
    "        --dat-list {dir_mscluster}/dat/mscluster_dat_list.txt \\\n",
    "        --output-name mscluster \\\n",
    "        --output-file-size 100000000 \\\n",
    "        --out-dir {dir_mscluster}/cluster_{i} \\\n",
    "        --model-dir ../bin/MsCluster/Models \\\n",
    "        --memory-gb 50 \\\n",
    "        --fragment-tolerance 0.05 \\\n",
    "        --precursor-ppm 20 \\\n",
    "        --assign-charges \\\n",
    "        --mixture-prob {mixture_prob} \\\n",
    "        --num-rounds {rounds} \\\n",
    "        --keep-dataset-idx\"\"\"\n",
    "    if not os.path.exists(os.path.join(dir_mscluster, f'cluster_{i}')):\n",
    "        ! eval {cmd}\n",
    "    # Evaluate clustering performance.\n",
    "    cluster_labels = get_clusters_mscluster(\n",
    "        os.path.join(dir_mscluster, f'cluster_{i}'), ids)\n",
    "    for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "        num_clustered, num_noise, \\\n",
    "            prop_clustered, prop_clustered_incorrect, \\\n",
    "            homogeneity, completeness = \\\n",
    "                evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                  max_cluster_size)\n",
    "        performance.append(('MS-Cluster', (mixture_prob, rounds),\n",
    "                            min_cluster_size, max_cluster_size,\n",
    "                            num_clustered, num_noise,\n",
    "                            prop_clustered, prop_clustered_incorrect,\n",
    "                            homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spectra_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_spectracluster = os.path.join(work_dir, 'spectra-cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p .../data/processed/spectra-cluster/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_spectracluster = {0: 0.99999, 1: 0.9999, 2: 0.999, 3: 0.99, 4: 0.95,\n",
    "                     5: 0.9, 6: 0.8, 7: 0.7}\n",
    "rounds = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, threshold_end in hp_spectracluster.items():\n",
    "    logging.info('spectra-cluster run %d (threshold_end=%.4f ; rounds=%d)',\n",
    "                 i + 1, threshold_end, rounds)\n",
    "    filename = os.path.join(dir_spectracluster, f'clusters_{i}.txt')\n",
    "    # Execute clustering.\n",
    "    cmd = f\"\"\"java -jar ../bin/spectra-cluster/spectra-cluster-cli-1.1.2.jar \\\n",
    "        ../data/interim/*.mgf \\\n",
    "        -binary_directory {dir_spectracluster}/tmp \\\n",
    "        -fast_mode \\\n",
    "        -fragment_tolerance 0.05 \\\n",
    "        -keep_binary_files \\\n",
    "        -major_peak_jobs $(nproc --all) \\\n",
    "        -output_path {filename} \\\n",
    "        -precursor_tolerance 20 \\\n",
    "        -precursor_tolerance_unit ppm \\\n",
    "        -reuse_binary_files \\\n",
    "        -rounds {rounds} \\\n",
    "        -threshold_end {threshold_end} \\\n",
    "        -threshold_start 1.0 \\\n",
    "        -x_disable_mgf_comments\"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        ! eval {cmd}\n",
    "    # Evaluate clustering performance.\n",
    "    cluster_labels = get_clusters_spectracluster(filename, ids)\n",
    "    for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "        num_clustered, num_noise, \\\n",
    "            prop_clustered, prop_clustered_incorrect, \\\n",
    "            homogeneity, completeness = \\\n",
    "                evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                  max_cluster_size)\n",
    "        performance.append(('spectra-cluster', (threshold_end, rounds),\n",
    "                            min_cluster_size, max_cluster_size,\n",
    "                            num_clustered, num_noise,\n",
    "                            prop_clustered, prop_clustered_incorrect,\n",
    "                            homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_falcon = os.path.join(work_dir, 'falcon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ../data/processed/falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_falcon = {0: 0.05, 1: 0.1, 2: 0.15, 3: 0.2, 4: 0.25, 5: 0.3, 6: 0.35}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.work_dir = dir_falcon\n",
    "for i, eps in hp_falcon.items():\n",
    "    logging.info('hp_falcon run %d (eps=%.2f)', i + 1, eps)\n",
    "    filename = os.path.join(dir_falcon, f'clusters_{i}.csv')\n",
    "    config.eps = eps\n",
    "    # Execute clustering.\n",
    "    if not os.path.isfile(filename):\n",
    "        falcon.main()\n",
    "        os.rename(os.path.join(dir_falcon, 'clusters.csv'), filename)\n",
    "    # Evaluate clustering performance.\n",
    "    cluster_labels = get_clusters_falcon(filename, ids)\n",
    "    for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "        num_clustered, num_noise, \\\n",
    "            prop_clustered, prop_clustered_incorrect, \\\n",
    "            homogeneity, completeness = \\\n",
    "                evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                  max_cluster_size)\n",
    "        performance.append(('falcon', eps,\n",
    "                            min_cluster_size, max_cluster_size,\n",
    "                            num_clustered, num_noise,\n",
    "                            prop_clustered, prop_clustered_incorrect,\n",
    "                            homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaRaCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_maracluster = os.path.join(work_dir, 'maracluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ../data/processed/maracluster\n",
    "! ls -1 ../data/interim/*.mgf > ../data/processed/maracluster/files.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_maracluster = {0: -3.0, 1: -5.0, 2: -10.0, 3: -15.0, 4: -20.0, 5: -25.0,\n",
    "                  6: -30.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pval_threshold in hp_maracluster.items():\n",
    "    logging.info('MaRaCluster run %d (p-value threshold=%.1f)',\n",
    "                 i + 1, pval_threshold)\n",
    "    filename = os.path.join(\n",
    "        dir_maracluster,\n",
    "        f'MaRaCluster{i}.clusters_p{abs(int(pval_threshold))}.tsv')\n",
    "    # Execute clustering.\n",
    "    cmd = f\"\"\"../bin/maracluster-v1-01-linux-amd64/bin/maracluster batch \\\n",
    "        --batch ../data/processed/maracluster/files.txt \\\n",
    "        --output-folder ../data/processed/maracluster \\\n",
    "        --precursorTolerance 20ppm \\\n",
    "        --pvalThreshold {pval_threshold} \\\n",
    "        --clusterThresholds {pval_threshold} \\\n",
    "        --prefix MaRaCluster{i}\"\"\"\n",
    "    if not os.path.isfile(filename):\n",
    "        ! eval {cmd}\n",
    "    # Evaluate clustering performance.\n",
    "    cluster_labels = get_clusters_maracluster(filename, ids)\n",
    "    for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "        num_clustered, num_noise, \\\n",
    "            prop_clustered, prop_clustered_incorrect, \\\n",
    "            homogeneity, completeness = \\\n",
    "                evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                  max_cluster_size)\n",
    "        performance.append(('MaRaCluster', (pval_threshold,),\n",
    "                            min_cluster_size, max_cluster_size,\n",
    "                            num_clustered, num_noise,\n",
    "                            prop_clustered, prop_clustered_incorrect,\n",
    "                            homogeneity, completeness))\n",
    "    os.rename(filename, os.path.join(\n",
    "        dir_maracluster, f'PXD000561_maracluster_{i}.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### msCRUSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_mscrush = os.path.join(work_dir, 'mscrush')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ../data/processed/mscrush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_mscrush = {i: hp for i, hp in enumerate(itertools.product(\n",
    "    [50, 100, 200], [10, 15], [0.55, 0.65, 0.75]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (it, h, sim) in hp_mscrush.items():\n",
    "    logging.info('msCRUSH run %d (iteration=%d, hash=%d, similarity=%.2f)',\n",
    "                 i + 1, it, h, sim)\n",
    "    filename = os.path.join(dir_mscrush, f'PXD000561_mscrush_{i}',\n",
    "                            f'mscrush-c{config.charges[0]}.txt')\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        os.makedirs(os.path.dirname(filename))\n",
    "    # Execute clustering.\n",
    "    cmd = f\"\"\"../bin/mscrush/mscrush_on_general_charge \\\n",
    "        --files ../data/interim/*.mgf \\\n",
    "        --iteration {it} \\\n",
    "        --hash {h} \\\n",
    "        --thread $(nproc --all) \\\n",
    "        --similarity {sim} \\\n",
    "        --clustering_prefix {os.path.dirname(filename)}/mscrush\"\"\"\n",
    "    if not os.path.isfile(filename.format(config.charges[0])):\n",
    "        ! eval {cmd}\n",
    "    # Evaluate clustering performance.\n",
    "    cluster_labels = get_clusters_mscrush(filename, ids)\n",
    "    for min_cluster_size, max_cluster_size in min_cluster_sizes:\n",
    "        num_clustered, num_noise, \\\n",
    "            prop_clustered, prop_clustered_incorrect, \\\n",
    "            homogeneity, completeness = \\\n",
    "                evaluate_clusters(cluster_labels, min_cluster_size,\n",
    "                                  max_cluster_size)\n",
    "        performance.append(('msCRUSH', (it, h, sim),\n",
    "                            min_cluster_size, max_cluster_size,\n",
    "                            num_clustered, num_noise,\n",
    "                            prop_clustered, prop_clustered_incorrect,\n",
    "                            homogeneity, completeness))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare clustering results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(performance, columns=[\n",
    "    'tool', 'hyperparameters',\n",
    "    'min_cluster_size', 'max_cluster_size',\n",
    "    'num_clustered', 'num_noise',\n",
    "    'prop_clustered', 'prop_clustered_incorrect',\n",
    "    'homogeneity', 'completeness'])\n",
    "performance.to_csv('cluster_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More detailed analysis of cluster size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_clusters = {}\n",
    "\n",
    "# Read clustering results from the different tools.\n",
    "tool_clusters['falcon'] = get_clusters_falcon(\n",
    "    '../data/processed/falcon/clusters_1.csv', ids)\n",
    "tool_clusters['MS-Cluster'] = get_clusters_mscluster(\n",
    "    '../data/processed/mscluster/cluster_0', ids)\n",
    "tool_clusters['spectra-cluster'] = get_clusters_spectracluster(\n",
    "    '../data/processed/spectra-cluster/clusters_0.txt', ids)\n",
    "\n",
    "# Remove singleton and noise clusters.\n",
    "min_cluster_size = 2\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    # Only consider clusters with specific minimum (inclusive) size.\n",
    "    cluster_counts = clusters['cluster'].value_counts(dropna=False)\n",
    "    if min_cluster_size is not None:\n",
    "        clusters.loc[clusters['cluster'].isin(cluster_counts[\n",
    "            cluster_counts < min_cluster_size].index), 'cluster'] = -1\n",
    "\n",
    "    # Use consecutive cluster labels, skipping the noise points.    \n",
    "    cluster_map = (clusters['cluster'].value_counts(dropna=False)\n",
    "                   .drop(index=-1).to_frame().reset_index().reset_index()\n",
    "                   .rename(columns={'index': 'old', 'level_0': 'new'})\n",
    "                   .set_index('old')['new'])\n",
    "    cluster_map = cluster_map.to_dict(collections.defaultdict(lambda: -1))\n",
    "    clusters['cluster'] = clusters['cluster'].map(cluster_map)\n",
    "    num_clusters = clusters['cluster'].max() + 1\n",
    "\n",
    "    # Reassign noise points to singleton clusters.\n",
    "    noise_mask = clusters['cluster'] == -1\n",
    "    num_noise = noise_mask.sum()\n",
    "    clusters.loc[noise_mask, 'cluster'] = np.arange(\n",
    "        num_clusters, num_clusters + num_noise)\n",
    "    \n",
    "    tool_clusters[tool] = clusters\n",
    "    \n",
    "# Add cluster sizes.\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    cluster_counts = (clusters['cluster']\n",
    "                      .value_counts(dropna=False)\n",
    "                      .to_frame()\n",
    "                      .reset_index()\n",
    "                      .rename(columns={'index': 'cluster', 'cluster': 'size'}))\n",
    "    tool_clusters[tool] = pd.merge(clusters, cluster_counts, on='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of clusters per tool:')\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    print(f'- {tool}: {clusters[clusters[\"size\"] > 1][\"cluster\"].nunique():,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_peptide = tool_clusters['falcon']['sequence'].value_counts()\n",
    "max_peptide, num_max_peptide = max_peptide.index.values[0], max_peptide.values[0]\n",
    "print(f'Most frequent peptide: {max_peptide} is observed '\n",
    "      f'{num_max_peptide:,d} times')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_size_intervals = [(2, 5), (5, 20), (20, 100), (100, 500),\n",
    "                          (500, 5000), (5000, None)]\n",
    "max_peptide_clusters = []\n",
    "for tool, clusters in tool_clusters.items():\n",
    "    for cluster in (clusters[clusters['sequence'] == max_peptide]\n",
    "                    ['cluster'].unique()):\n",
    "        sequence_counts = (clusters[clusters['cluster'] == cluster]\n",
    "                           ['sequence'].value_counts(dropna=False))\n",
    "        if sequence_counts.sum() < cluster_size_intervals[0][0]:\n",
    "            continue\n",
    "        num_correct = sequence_counts[max_peptide]\n",
    "        num_unidentified = (sequence_counts[np.NaN]\n",
    "                            if np.NaN in sequence_counts else 0)\n",
    "        num_incorrect = sequence_counts.sum() - num_correct - num_unidentified\n",
    "        # Only consider clusters where this is the majority peptide.\n",
    "        if num_correct > num_incorrect:\n",
    "            cluster_size = num_correct + num_incorrect + num_unidentified\n",
    "            for min_interval_size, max_interval_size in cluster_size_intervals:\n",
    "                if (max_interval_size is None\n",
    "                        and min_interval_size <= cluster_size):\n",
    "                    interval = f'{min_interval_size}+'\n",
    "                    break\n",
    "                elif min_interval_size <= cluster_size < max_interval_size:\n",
    "                    interval = f'{min_interval_size}–{max_interval_size}'\n",
    "                    break\n",
    "            max_peptide_clusters.append((tool, interval, num_correct,\n",
    "                                         num_unidentified, num_incorrect))\n",
    "max_peptide_clusters = (\n",
    "    pd.DataFrame(max_peptide_clusters, columns=[\n",
    "        'tool', 'interval', 'num_correct', 'num_unidentified', 'num_incorrect'])\n",
    "    .sort_values(['tool', 'interval'], key=natsort.natsort_keygen()))\n",
    "max_peptide_clusters['num_total'] = (max_peptide_clusters['num_correct'] +\n",
    "                                     max_peptide_clusters['num_unidentified'] +\n",
    "                                     max_peptide_clusters['num_incorrect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of the top 5 largest clusters for peptide {max_peptide} per tool:')\n",
    "(max_peptide_clusters.groupby('tool')\n",
    " ['num_total'].apply(lambda x: x.sort_values(ascending=False).head())\n",
    " .to_frame().droplevel(1).reset_index()\n",
    " .rename(columns={'num_total': 'largest clusters'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique clusters for peptide {max_peptide} per tool:')\n",
    "for tool, num_clusters in max_peptide_clusters['tool'].value_counts().iteritems():\n",
    "    print(f'- {tool}: {num_clusters}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 7\n",
    "height = width / 1.618\n",
    "fig, axes = plt.subplots(2, 2, figsize=(width * 2, height * 2))\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "# Number of clustered spectra and completeness.\n",
    "for tool in ('falcon', 'MS-Cluster', 'spectra-cluster'):\n",
    "    tool_performance = performance[(performance['tool'] == tool) &\n",
    "                                   (performance['min_cluster_size'] == 2)]\n",
    "    axes[0].plot(tool_performance['prop_clustered_incorrect'],\n",
    "                 tool_performance['prop_clustered'], marker='o', label=tool)\n",
    "    axes[1].plot(tool_performance['prop_clustered_incorrect'],\n",
    "                 tool_performance['completeness'], marker='o', label=tool)\n",
    "\n",
    "axes[0].set_xlim(0, 0.05)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[1].set_xlim(0, 0.05)\n",
    "axes[1].set_ylim(0.75, 1)\n",
    "\n",
    "axes[0].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[0].yaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "axes[1].xaxis.set_major_formatter(mticker.PercentFormatter(1, 0))\n",
    "\n",
    "axes[0].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[0].set_ylabel('Clustered spectra')\n",
    "axes[1].set_xlabel('Incorrectly clustered spectra')\n",
    "axes[1].set_ylabel('Completeness')\n",
    "\n",
    "axes[0].legend(loc='lower right', frameon=False)\n",
    "axes[1].legend(loc='lower right', frameon=False)\n",
    "\n",
    "# Cluster sizes.\n",
    "max_size = max([clusters['size'].max()\n",
    "                for clusters in tool_clusters.values()])\n",
    "for i, (tool, clusters) in enumerate(tool_clusters.items()):\n",
    "    sizes = np.insert(clusters['size'].values, -1, max_size)\n",
    "    sns.ecdfplot(sizes, stat='proportion', ax=axes[2], label=tool,\n",
    "                 zorder=len(tool_clusters) - i)\n",
    "    \n",
    "axes[2].set_xscale('log')\n",
    "axes[2].set_ylim(0., 1.01)\n",
    "\n",
    "axes[2].yaxis.set_major_formatter(mticker.PercentFormatter(1))\n",
    "\n",
    "axes[2].set_xlabel('Cluster size')\n",
    "axes[2].set_ylabel('Cumulative clustered spectra')\n",
    "\n",
    "axes[2].legend(loc='lower right', frameon=False)\n",
    "\n",
    "# Frequent peptide.\n",
    "max_peptide_clusters_grouped = pd.concat(\n",
    "    [max_peptide_clusters,\n",
    "     pd.DataFrame([(tool, interval, 0, 0, 0)\n",
    "                   for tool in max_peptide_clusters['tool'].unique()\n",
    "                   for interval in max_peptide_clusters['interval'].unique()],\n",
    "                  columns=['tool', 'interval', 'num_correct',\n",
    "                           'num_unidentified', 'num_incorrect'])],\n",
    "    ignore_index=True)    \n",
    "max_peptide_clusters_grouped = (max_peptide_clusters_grouped\n",
    "                                .sort_values(['tool', 'interval'],\n",
    "                                             key=natsort.natsort_keygen())\n",
    "                                .groupby(['tool', 'interval'], sort=False)\n",
    "                                [['num_correct', 'num_unidentified',\n",
    "                                  'num_incorrect']].apply(sum)\n",
    "                                .reset_index())\n",
    "\n",
    "for position, tool, hatch in zip([1.5, 0.5, -0.5],\n",
    "                                 ['falcon', 'MS-Cluster', 'spectra-cluster'],\n",
    "                                 ['x', '', '.']):\n",
    "    (max_peptide_clusters_grouped[max_peptide_clusters_grouped['tool'] == tool]\n",
    "     .plot.bar(x='interval', position=position, rot=0, stacked=True,\n",
    "               width=0.2, edgecolor='black', hatch=3 * hatch, ax=axes[3]))\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#6da7de', edgecolor='black',label=max_peptide),\n",
    "    Patch(facecolor='white', edgecolor='black', hatch=3 * 'x',\n",
    "          label='falcon'),\n",
    "    Patch(facecolor='#9e0059', edgecolor='black', label='Unidentified'),\n",
    "    Patch(facecolor='white', edgecolor='black', label='MS-Cluster'),\n",
    "    Patch(facecolor='#dee000', edgecolor='black', label='Incorrect peptide'),\n",
    "    Patch(facecolor='white', edgecolor='black', hatch=3 * '.',\n",
    "          label='spectra-cluster')]\n",
    "axes[3].legend(handles=legend_elements, loc='center',\n",
    "               bbox_to_anchor=(0.5, 1.1), ncol=3, frameon=False)\n",
    "\n",
    "axes[3].set_xlim(-0.5, axes[3].get_xlim()[1])\n",
    "\n",
    "axes[3].yaxis.set_major_formatter(mticker.StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "axes[3].set_xlabel('Cluster size')\n",
    "axes[3].set_ylabel('Clustered spectra')\n",
    "\n",
    "for i, (ax, c) in enumerate(zip(axes, 'ABCD')):\n",
    "    ax.annotate(c, xy=(-0.15, 1.1), xycoords='axes fraction',\n",
    "                fontsize='xx-large', weight='bold')\n",
    "\n",
    "for ax in axes:\n",
    "    sns.despine(ax=ax)\n",
    "    \n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('cluster_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
